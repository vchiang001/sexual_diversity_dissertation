{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhrYaRTCnn2KINcqy590Lu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vchiang001/sexual_diversity_dissertation/blob/main/simba_classifiers_performance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4326EjlJb7eQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "manual_mi_csv = \"/content/drive/MyDrive/BEHAVIOMICS/SpecificAim1/manualannot_mi.csv\"\n",
        "machine_directory = '/content/drive/MyDrive/BEHAVIOMICS/SpecificAim1/lowres_simba_transformed'"
      ],
      "metadata": {
        "id": "-ssueA_gcVFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extract the desired data from simba csv files\n",
        "manual_mi = pd.read_csv(manual_mi_csv)\n",
        "machine_mi = pd.DataFrame()\n",
        "\n",
        "for file_name in os.listdir(machine_directory):\n",
        "    if file_name.endswith(\".csv\"):\n",
        "        print(file_name)\n",
        "        df = pd.read_csv(os.path.join(machine_directory, file_name))\n",
        "        extracted_data = df.loc[:8516, \"mount_intromit\"]\n",
        "        print(extracted_data)\n",
        "        extracted_data = pd.DataFrame(extracted_data)\n",
        "        specific_heading = file_name.split(\"/\")[-1].split(\".csv\")[0]\n",
        "        print(specific_heading)\n",
        "        extracted_data.columns = [specific_heading]\n",
        "        machine_mi[specific_heading] = extracted_data\n",
        "\n",
        "# Display the combined dataframe\n",
        "print(machine_mi)"
      ],
      "metadata": {
        "id": "KuiR9pl-cZrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate precision & recall\n",
        "evaluation_metrics = {}\n",
        "for i in range(10):  # assuming both dataframes have 10 columns\n",
        "    true_labels = manual_mi.iloc[:, i]\n",
        "    predicted_labels = machine_mi.iloc[:, i]\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    precision = precision_score(true_labels, predicted_labels)\n",
        "    recall = recall_score(true_labels, predicted_labels)\n",
        "    f1 = f1_score(true_labels, predicted_labels)\n",
        "\n",
        "    evaluation_metrics[f'Column_{i+1}'] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1': f1}"
      ],
      "metadata": {
        "id": "9YRTJSdbcp3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving the precision & recall results\n",
        "for col, metrics in evaluation_metrics.items():\n",
        "    print(f'Evaluation metrics for {col}:')\n",
        "    for metric, value in metrics.items():\n",
        "        print(f'{metric}: {value}')\n",
        "    print()\n",
        "\n",
        "metrics_df = pd.DataFrame(evaluation_metrics).transpose()\n",
        "metrics_df.to_csv('/content/drive/MyDrive/BEHAVIOMICS/SpecificAim1/mi_evaluation_metrics.csv')\n"
      ],
      "metadata": {
        "id": "hxHO1QbVcznT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating differences of latency & duration\n",
        "differences = []\n",
        "\n",
        "for col_index in range(len(manual_mi.columns)):\n",
        "    print('col_index')\n",
        "    print(col_index)\n",
        "    col = manual_mi.columns[col_index]\n",
        "    print('col')\n",
        "    print(col)\n",
        "\n",
        "    latency = manual_mi.iloc[:, col_index].idxmax()\n",
        "    print(latency)\n",
        "    print('latency')\n",
        "\n",
        "    duration = manual_mi.iloc[:, col_index].sum()\n",
        "    print('duration')\n",
        "    print(duration)\n",
        "\n",
        "    machine_mi_latency = machine_mi.iloc[:, col_index].idxmax()\n",
        "    print('machine_mi_latency')\n",
        "    print(machine_mi_latency)\n",
        "\n",
        "    machine_mi_duration = machine_mi.iloc[:, col_index].sum()\n",
        "    print('machine_mi_duration')\n",
        "    print(machine_mi_duration)\n",
        "\n",
        "    latency_difference = latency - machine_mi_latency\n",
        "    print(latency_difference)\n",
        "    print('latency_difference')\n",
        "    duration_difference = duration - machine_mi_duration\n",
        "    print('duration_difference')\n",
        "    print(duration_difference)\n",
        "\n",
        "    differences.append([latency_difference, duration_difference])"
      ],
      "metadata": {
        "id": "mJ9eYIXIc87N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving the difference of latency & duration results\n",
        "difference_df = pd.DataFrame(differences, columns=['latency_difference', 'duration_difference'])\n",
        "difference_df.to_csv('/content/drive/MyDrive/BEHAVIOMICS/SpecificAim1/mi_differences.csv', index=False)"
      ],
      "metadata": {
        "id": "Eo6izQBCdIPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#do the same for EJ data\n",
        "manual_ej_csv = \"/content/drive/MyDrive/BEHAVIOMICS/SpecificAim1/manualannot_ej.csv\""
      ],
      "metadata": {
        "id": "1R-tZ0E1dTSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extract EJ data\n",
        "manual_ej = pd.read_csv(manual_ej_csv)\n",
        "machine_ej = pd.DataFrame()\n",
        "\n",
        "for file_name in os.listdir(machine_directory):\n",
        "    if file_name.endswith(\".csv\"):\n",
        "        print(file_name)\n",
        "        df = pd.read_csv(os.path.join(machine_directory, file_name))\n",
        "        extracted_data = df.loc[:8516, \"ejaculate\"]\n",
        "        print(extracted_data)\n",
        "        extracted_data = pd.DataFrame(extracted_data)\n",
        "        specific_heading = file_name.split(\"/\")[-1].split(\".csv\")[0]\n",
        "        print(specific_heading)\n",
        "        extracted_data.columns = [specific_heading]\n",
        "        machine_ej[specific_heading] = extracted_data\n",
        "\n",
        "print(machine_ej)"
      ],
      "metadata": {
        "id": "QhGQkPdzdaST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating & saving the precision & recall results for EJ\n",
        "evaluation_metrics = {}\n",
        "for i in range(10):  # assuming both dataframes have 10 columns\n",
        "    true_labels = manual_ej.iloc[:, i]\n",
        "    predicted_labels = machine_ej.iloc[:, i]\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    precision = precision_score(true_labels, predicted_labels)\n",
        "    recall = recall_score(true_labels, predicted_labels)\n",
        "    f1 = f1_score(true_labels, predicted_labels)\n",
        "\n",
        "    evaluation_metrics[f'Column_{i+1}'] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1': f1}\n",
        "\n",
        "for col, metrics in evaluation_metrics.items():\n",
        "    print(f'Evaluation metrics for {col}:')\n",
        "    for metric, value in metrics.items():\n",
        "        print(f'{metric}: {value}')\n",
        "    print()\n",
        "\n",
        "metrics_df = pd.DataFrame(evaluation_metrics).transpose()\n",
        "metrics_df.to_csv('/content/drive/MyDrive/BEHAVIOMICS/SpecificAim1/ej_evaluation_metrics.csv')"
      ],
      "metadata": {
        "id": "Kb7sql5VdjZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating & saving the difference of latency & duration results\n",
        "differences = []\n",
        "\n",
        "for col_index in range(len(manual_mi.columns)):\n",
        "    print('col_index')\n",
        "    print(col_index)\n",
        "    col = manual_ej.columns[col_index]\n",
        "    print('col')\n",
        "    print(col)\n",
        "\n",
        "    latency = manual_ej.iloc[:, col_index].idxmax()\n",
        "    print(latency)\n",
        "    print('latency')\n",
        "\n",
        "    duration = manual_ej.iloc[:, col_index].sum()\n",
        "    print('duration')\n",
        "    print(duration)\n",
        "\n",
        "    machine_ej_latency = machine_ej.iloc[:, col_index].idxmax()\n",
        "    print('machine_ej_latency')\n",
        "    print(machine_ej_latency)\n",
        "\n",
        "    machine_ej_duration = machine_ej.iloc[:, col_index].sum()\n",
        "    print('machine_ej_duration')\n",
        "    print(machine_ej_duration)\n",
        "\n",
        "    latency_difference = latency - machine_ej_latency\n",
        "    print(latency_difference)\n",
        "    print('latency_difference')\n",
        "    duration_difference = duration - machine_ej_duration\n",
        "    print('duration_difference')\n",
        "    print(duration_difference)\n",
        "\n",
        "    differences.append([latency_difference, duration_difference])\n",
        "\n",
        "difference_df = pd.DataFrame(differences, columns=['latency_difference', 'duration_difference'])\n",
        "difference_df.to_csv('/content/drive/MyDrive/BEHAVIOMICS/SpecificAim1/ej_differences.csv', index=False)"
      ],
      "metadata": {
        "id": "lU6a1pcsdv5k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}