{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFPq1bqrsOkwE0fBYfg93O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vchiang001/sexual_diversity_dissertation/blob/main/Dijkstra_lab_African_cichlids_behavioural_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using existing trained DeepLabCut model to analyse videos https://github.com/DeepLabCut/DeepLabCut/blob/main/examples/COLAB/COLAB_maDLC_TrainNetwork_VideoAnalysis.ipynb\n"
      ],
      "metadata": {
        "id": "dZVB5RentP0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###ATTENTION: Variables to set###\n",
        "path_config_file = '/content/drive/My Drive/config.yaml' #set path to the config file to the project in https://drive.google.com/drive/folders/1ZPSlzhH2hSAqg3dB1TKuadsiGfDhfLMC?usp=sharing\n",
        "video_directory = '' #link to the video files you want to perform pose estimation\n",
        "video_type = '.MOV' #video file type\n"
      ],
      "metadata": {
        "id": "YMG6tdZPWMzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#(this will take a few minutes to install all the dependences!)\n",
        "!apt update && apt install cuda-11-8\n",
        "!pip install \"deeplabcut[tf]\"\n",
        "%reload_ext numpy\n",
        "%reload_ext scipy\n",
        "%reload_ext matplotlib\n",
        "%reload_ext mpl_toolkits\n",
        "!pip install --upgrade scikit-image\n",
        "!pip3 install pickle5"
      ],
      "metadata": {
        "id": "5q6QVuIqtc1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmDdeMC0tHzM"
      },
      "outputs": [],
      "source": [
        "#all imports\n",
        "import deeplabcut\n",
        "import pickle5 as pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now, let's link to your Google Drive. Run this cell and follow the authorization instructions:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "H7AirZs-tcXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#analyse the videos without animal assembly & tracking\n",
        "deeplabcut.analyze_videos(config=path_config_file,\n",
        "                          videos=video_directory,\n",
        "                          videotype=video_type,\n",
        "                          shuffle=1,\n",
        "                          save_as_csv=True,\n",
        "                          auto_track=False,\n",
        "                          n_tracks=None,\n",
        "                          identity_only=True,\n",
        "                          )"
      ],
      "metadata": {
        "id": "TLIpIltgX3JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualise the pose estimation results\n",
        "deeplabcut.create_video_with_all_detections(config=path_config_file,\n",
        "                                            videos=video_directory,\n",
        "                                            videotype=video_type,\n",
        "                                            shuffle=1,\n",
        "                                            displayedbodyparts='all',\n",
        "                                            )"
      ],
      "metadata": {
        "id": "LvKRZEHHZ0Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transform pose estimation output for downstream analysis of two-cichlids separated setup by a process called \"unpickling\""
      ],
      "metadata": {
        "id": "1fhsaIcId4_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###ATTENTION: run on laptop command prompt/terminal to find region of interest(ROI) for your video\n",
        "#colab doesn't support GUI, to define ROI in a randomly extracted frame of your video (should match the pickle file you set)\n",
        "\n",
        "#All imports\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Polygon\n",
        "\n",
        "#ROI Polygon function\n",
        "class ROIPolygon:\n",
        "    def __init__(self, image):\n",
        "        self.image = image\n",
        "        self.fig, self.ax = plt.subplots()\n",
        "        self.ax.set_title('Select ROI')\n",
        "        self.ax.imshow(self.image, cmap='gray')\n",
        "        self.ROI_coords = []\n",
        "\n",
        "    def select_roi(self):\n",
        "        print(\"Select points to define the ROI polygon. Press 'Enter' to finish.\")\n",
        "        self.ROI_coords = plt.ginput(n=-1, timeout=-1)\n",
        "        polygon = Polygon(self.ROI_coords, closed=True, fill=None, edgecolor='r')\n",
        "        self.ax.add_patch(polygon)\n",
        "        plt.show()\n",
        "\n",
        "###ATTENTION: Variables to set###\n",
        "video_path = r\"C:\\Users\\vscch\\OneDrive\\Desktop\\python\\2m_separated\\2m_separated5.mov\" #File path of your video\n",
        "\n",
        "#Load the video & randomly extracts a frame to find your ROI\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "random_frame_index = np.random.randint(0, total_frames)\n",
        "cap.set(cv2.CAP_PROP_POS_FRAMES, random_frame_index)\n",
        "ret, frame = cap.read()\n",
        "frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "cap.release()\n",
        "\n",
        "# Create an instance of ROIPolygon with the randomly extracted frame\n",
        "ROI = ROIPolygon(frame_rgb)\n",
        "ROI.select_roi()"
      ],
      "metadata": {
        "id": "l8oFCGItx-mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###ATTENTION: Variables to set###\n",
        "# Define the bounding box coordinates of which cichlid you want to analyse(x,y)\n",
        "coord1 = (61, 162)  # Top-left corner\n",
        "coord2 = (921, 306)  # Top-right corner\n",
        "coord3 = (940, 766)  # Bottom-right corner\n",
        "coord4 = (61, 886)  # Bottom-left corner\n",
        "#Record the coordinates you used into a sheet with the relevant file names, so you can refer back to it later & used to verify\n",
        "\n",
        "#pose estimation output from DeepLabCut for one video (with file name _full.pickle)\n",
        "file_path = '/content/drive/MyDrive/BEHAVIOMICS/SpecificAim2/202404_unpickle/6-J1-1115_butts1DLC_resnet50_Astatotilapia_burtoniFeb17shuffle1_100000_full.pickle'\n",
        "directory_path = '/content/drive/MyDrive/BEHAVIOMICS/SpecificAim2/202404_unpickle' #location of where you want to save your unpickled file"
      ],
      "metadata": {
        "id": "7HC3RFEOx6tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Can be used to verify the location of your polygon is correct - use on your laptop\n",
        "\n",
        "coord1 = (61, 162)  # Top-left corner\n",
        "coord2 = (921, 306)  # Top-right corner\n",
        "coord3 = (940, 766)  # Bottom-right corner\n",
        "coord4 = (61, 886)  # Bottom-left corner\n",
        "\n",
        "# Create a polygon representing the slanted bounding box\n",
        "bounding_box_polygon = Polygon([coord1, coord2, coord3, coord4])\n",
        "\n",
        "# Load the video & randomly extracts a frame to find your ROI\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "random_frame_index = np.random.randint(0, total_frames)\n",
        "cap.set(cv2.CAP_PROP_POS_FRAMES, random_frame_index)\n",
        "ret, frame = cap.read()\n",
        "cap.release()\n",
        "\n",
        "# Convert the frame to RGB\n",
        "frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Draw the polygon on the frame\n",
        "pts = np.array(bounding_box_polygon.exterior.coords, np.int32)\n",
        "pts = pts.reshape((-1,1,2))\n",
        "cv2.polylines(frame_rgb,[pts],True,(255,0,0),2) # Draw the bounding box polygon on the frame\n",
        "\n",
        "# Display the frame with the polygon drawn\n",
        "cv2.imshow('Frame with Bounding Box', frame_rgb)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "dI9_BRJ-O963"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#All imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.signal import savgol_filter\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.widgets import RectangleSelector\n",
        "from shapely.geometry import Polygon, Point\n",
        "import os"
      ],
      "metadata": {
        "id": "_t4rF2KgxswK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a polygon representing the slanted bounding box\n",
        "print('   Unpickling', file_path)\n",
        "df = pd.read_pickle(file_path)\n",
        "print('   Read pickle file with', len(df), 'frames containing detections')\n",
        "\n",
        "body_parts = [\n",
        "    'eyes_left',\n",
        "    'eyes_right',\n",
        "    'mouthend_left',\n",
        "    'mouthend_right',\n",
        "    'caudalfin_top_base',\n",
        "    'caudalfin_bottom_base',\n",
        "    'caudalfin_top_end',\n",
        "    'caudalfin_bottom_end',\n",
        "    'dorsal_opercular',\n",
        "    'dorsal_opercular_mid',\n",
        "    'dorsal_mid',\n",
        "    'dorsal_midbase',\n",
        "    'ventral_opercular',\n",
        "    'ventral_opercular_mid',\n",
        "    'ventral_mid',\n",
        "    'ventral_midbase',\n",
        "    'pectoralfin_base_left',\n",
        "    'pectoralfin_base_right',\n",
        "]\n",
        "\n",
        "bounding_box_polygon = Polygon([coord1, coord2, coord3, coord4]) # Create a polygon representing the slanted bounding box\n",
        "columns = []\n",
        "columns = ['frame']\n",
        "for part in body_parts:\n",
        "  columns.append(part + '_x')\n",
        "  print(\"x:\", part + '_x')\n",
        "  columns.append(part + '_y')\n",
        "  print(\"y:\", part + '_y')\n",
        "  columns.append(part + '_likelihood')\n",
        "  print(\"likelihood:\", part + '_likelihood')\n",
        "\n",
        "columns"
      ],
      "metadata": {
        "id": "IxOUL2N3yOPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Unpickle the files & save the output as a csv with original name + unpickle\n",
        "\n",
        "output = []\n",
        "empty_cell_val = 'NA'\n",
        "\n",
        "counter = 0\n",
        "for k, v in df.items():\n",
        "    counter += 1\n",
        "    if not k.startswith('frame'):\n",
        "        continue\n",
        "    row = k[5:]\n",
        "\n",
        "    data_row = dict.fromkeys(columns)\n",
        "    data_row['frame'] = row\n",
        "\n",
        "    # Iterate over both coordinates and confidence values simultaneously\n",
        "    for bp, (coord_arr, conf_arr) in enumerate(zip(v['coordinates'][0][:18], v['confidence'])):\n",
        "        for p, (xy_coords, conf) in enumerate(zip(coord_arr, conf_arr)):\n",
        "            point = Point(xy_coords[0], xy_coords[1])\n",
        "            if bounding_box_polygon.contains(point):\n",
        "                # Check if the x-coordinate is empty or if the new confidence is higher\n",
        "                if data_row[body_parts[bp] + '_x'] is None:\n",
        "                    # If x-coordinate is empty, set the values\n",
        "                    data_row[body_parts[bp] + '_x'] = xy_coords[0]\n",
        "                    data_row[body_parts[bp] + '_y'] = xy_coords[1]\n",
        "                    data_row[body_parts[bp] + '_likelihood'] = conf[0]\n",
        "                else:\n",
        "                    # If x-coordinate is not empty, compare the new confidence\n",
        "                    existing_confidence = data_row[body_parts[bp] + '_likelihood']\n",
        "                    if conf[0] > existing_confidence:\n",
        "                        print(row, \"replacing point with higher confidence\")\n",
        "                        # If new confidence is higher, update the values\n",
        "                        data_row[body_parts[bp] + '_x'] = xy_coords[0]\n",
        "                        data_row[body_parts[bp] + '_y'] = xy_coords[1]\n",
        "                        data_row[body_parts[bp] + '_likelihood'] = conf[0]\n",
        "    output.append(data_row)\n",
        "\n",
        "    if counter > 10000000:\n",
        "        print('   Hit the maximum of 10 MILLION frames..')\n",
        "        break\n",
        "\n",
        "output_file = pd.DataFrame.from_dict(output)\n",
        "output_file = output_file.set_index('frame')\n",
        "output_file.fillna(empty_cell_val, inplace=True)\n",
        "output_file = output_file.sort_values(by='frame')\n",
        "\n",
        "output_file_name = file_path.split('.')[-2].split(os.sep)[-1] + '_UNPICKLED.csv'\n",
        "\n",
        "output_path = directory_path + os.sep + output_file_name\n",
        "print('   Writing output to:', output_path)\n",
        "\n",
        "output_file.to_csv(output_path)\n",
        "print('   Done unpickling file!')"
      ],
      "metadata": {
        "id": "5syX5yWpyUCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###ATTENTION: Double check each output has pose data that corresponds to your video results, before moving on."
      ],
      "metadata": {
        "id": "az8XHaTEVmdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transform unpickled file with compatible headers for csv & h5 file types"
      ],
      "metadata": {
        "id": "A3FRC_yZekHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before starting this section\n",
        "\n",
        "\n",
        "1.   Compile all the unpickled files so that you can run all of them together using the code this section which can automatically apply to all csv files in a folder\n",
        "2.   Since unpickle file names are now super long (due to how DeepLabCut spits out file outputs), I would recommend renaming them to a shorter version, so it's easier to know which files you are working with\n",
        "\n"
      ],
      "metadata": {
        "id": "mebkO7bgI3yH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###ATTENTION: Variables to set###\n",
        "scorer = 'vscc'\n",
        "input_folder = '/path/to/input_folder/' #folder where you saved all the unpickled files\n",
        "output_folder = '/content/drive/MyDrive/BEHAVIOMICS/SpecificAim2/202404_unpickle'\n"
      ],
      "metadata": {
        "id": "AVuBy_kVd7ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#All imports\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "sdSKsb0AeaG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defines body parts of our cichlids, and create functions for correct formatting of files\n",
        "body_parts = [\n",
        "    'eyes_left',\n",
        "    'eyes_right',\n",
        "    'mouthend_left',\n",
        "    'mouthend_right',\n",
        "    'caudalfin_top_base',\n",
        "    'caudalfin_bottom_base',\n",
        "    'caudalfin_top_end',\n",
        "    'caudalfin_bottom_end',\n",
        "    'dorsal_opercular',\n",
        "    'dorsal_opercular_mid',\n",
        "    'dorsal_mid',\n",
        "    'dorsal_midbase',\n",
        "    'ventral_opercular',\n",
        "    'ventral_opercular_mid',\n",
        "    'ventral_mid',\n",
        "    'ventral_midbase',\n",
        "    'pectoralfin_base_left',\n",
        "    'pectoralfin_base_right',\n",
        "]\n",
        "\n",
        "def correct_header(df, scorer, body_parts, output_folder):\n",
        "    formatted_file_name = os.path.splitext(os.path.basename(df))[0] + '_formatted.csv'\n",
        "    output_file_path = os.path.join(output_folder, formatted_file_name)\n",
        "    columns = pd.MultiIndex.from_product([[scorer], body_parts, ['x', 'y', 'likelihood']], names=['scorer', 'bodyparts', 'coords'])\n",
        "    data = pd.read_csv(df, index_col=0)\n",
        "    data.index.name = None\n",
        "    data.reset_index(drop=True, inplace=True)\n",
        "    data.columns = columns\n",
        "    data.to_csv(output_file_path)\n",
        "    guarantee_multiindex_rows(data)\n",
        "    data.to_hdf(output_file_path.replace(\".csv\", \".h5\"), key=\"df_with_missing\", mode=\"w\")\n",
        "    return output_file_path\n",
        "\n",
        "def guarantee_multiindex_rows(df):\n",
        "    if not isinstance(df.index, pd.MultiIndex):\n",
        "        path = df.index[0]\n",
        "        try:\n",
        "            sep = \"/\" if \"/\" in path else \"\\\\\"\n",
        "            splits = tuple(df.index.str.split(sep))\n",
        "            df.index = pd.MultiIndex.from_tuples(splits)\n",
        "        except TypeError:\n",
        "            pass\n",
        "    try:\n",
        "        df.index = df.index.set_levels(df.index.levels[1].astype(str), level=1)\n",
        "    except AttributeError:\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "zzsFsRjzJY7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over all CSV files in the input folder & saves CSV & H5 with correct format to another folder\n",
        "for file_name in os.listdir(input_folder):\n",
        "    if file_name.endswith('.csv'):\n",
        "        file_path = os.path.join(input_folder, file_name)\n",
        "        formatted_file_path = correct_header(file_path, scorer, body_parts, output_folder)\n",
        "        #data = pd.read_csv(formatted_file_path, index_col=0)\n",
        "        #guarantee_multiindex_rows(data)\n",
        "        #data.to_hdf(formatted_file_path.replace(\".csv\", \".h5\"), key=\"df_with_missing\", mode=\"w\")"
      ],
      "metadata": {
        "id": "hW3Ow4CUJaki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###ATTENTION: to make subsequent steps easier, create a folder 'h5' and 'csv' and save the relevant file types into the folder.\n",
        "###Also double check that the formatted files has the same content as your unpickled files before moving on."
      ],
      "metadata": {
        "id": "oNxqBVK0HnuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calculating velocity, acceleration, speed using DLC2kinematics https://github.com/AdaptiveMotorControlLab/DLC2Kinematics"
      ],
      "metadata": {
        "id": "6DC06ED5wcoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install dlc2kinematics: when it asks if the session should be restarted, you should do so\n",
        "!pip install dlc2kinematics"
      ],
      "metadata": {
        "id": "OlKw7oWSwnsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###ATTENTION: Variables to set###\n",
        "input_folder = '/path/to/input_folder/' #folder where you saved all the H5 files to analyse\n",
        "output_folder = '/content/drive/MyDrive/BEHAVIOMICS/SpecificAim2/202404_unpickle'"
      ],
      "metadata": {
        "id": "eO9nET-MKM-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#All imports\n",
        "import dlc2kinematics\n",
        "import os"
      ],
      "metadata": {
        "id": "LEeRzsw1wqeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def velocity_acceleration_speed(file_path, output_folder):\n",
        "    # Load data from the H5 file\n",
        "    df, bodyparts, scorer = dlc2kinematics.load_data(file_path)\n",
        "\n",
        "    # Compute velocity and save to CSV\n",
        "    df_vel = dlc2kinematics.compute_velocity(df, bodyparts=['all'])\n",
        "    vel_csv_path = os.path.join(output_folder, os.path.splitext(os.path.basename(file_path))[0] + '_vel.csv')\n",
        "    df_vel.to_csv(vel_csv_path, index=True)\n",
        "\n",
        "    # Compute acceleration and save to CSV\n",
        "    df_acc = dlc2kinematics.compute_acceleration(df, bodyparts=['all'])\n",
        "    acc_csv_path = os.path.join(output_folder, os.path.splitext(os.path.basename(file_path))[0] + '_acc.csv')\n",
        "    df_acc.to_csv(acc_csv_path, index=True)\n",
        "\n",
        "    # Compute speed and save to CSV\n",
        "    df_speed = dlc2kinematics.compute_speed(df, bodyparts=['all'])\n",
        "    speed_csv_path = os.path.join(output_folder, os.path.splitext(os.path.basename(file_path))[0] + '_speed.csv')\n",
        "    df_speed.to_csv(speed_csv_path, index=True)"
      ],
      "metadata": {
        "id": "MO_9VeXaKAo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over all H5 files in the input folder and save files to output folder\n",
        "for file_name in os.listdir(input_folder):\n",
        "    if file_name.endswith('.h5'):\n",
        "        # Construct the full file path\n",
        "        file_path = os.path.join(input_folder, file_name)\n",
        "        # Process the H5 file\n",
        "        velocity_acceleration_speed(file_path, output_folder)"
      ],
      "metadata": {
        "id": "h9z7EsObLKPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plotting xy coordinates occupied https://github.com/DeepLabCut/DLCutils/blob/master/Demo_loadandanalyzeDLCdata.ipynb"
      ],
      "metadata": {
        "id": "9vppLJfcNCQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the toolbox (takes several seconds)\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "8roKPBqvNSCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###ATTENTION: Variables to set###\n",
        "folder_path = '/content/drive/MyDrive/BEHAVIOMICS/SpecificAim2/20240502_two_cichlid_analyses/formatted'\n",
        "output_folder = '/content/drive/MyDrive/BEHAVIOMICS/SpecificAim2/20240502_two_cichlid_analyses/xy_coord'"
      ],
      "metadata": {
        "id": "FvJROTkskWIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_xy_from_h5(file_path, output_folder):\n",
        "    # Load data from H5 file into a DataFrame\n",
        "    df = pd.read_hdf(file_path)\n",
        "\n",
        "    # Get unique body parts\n",
        "    bodyparts = df.columns.get_level_values(1).unique()\n",
        "\n",
        "    # Set up the figure and colors\n",
        "    fs = (10, 6)  # Example figsize\n",
        "    plt.figure(figsize=fs)\n",
        "    colors = plt.cm.get_cmap('jet', len(bodyparts))\n",
        "\n",
        "    # Plot the data points for each body part\n",
        "    scorer = df.columns.get_level_values(0)[0]  # Get the scorer name\n",
        "    for bpindex, bp in enumerate(bodyparts):\n",
        "        Index = df[scorer][bp]['likelihood'].values > 0.1\n",
        "        plt.plot(df[scorer][bp]['x'].values[Index], df[scorer][bp]['y'].values[Index], '.', color=colors(bpindex), alpha=0.2)\n",
        "\n",
        "    plt.gca().invert_yaxis()\n",
        "\n",
        "    # Create the colorbar\n",
        "    sm = plt.cm.ScalarMappable(cmap=plt.get_cmap('jet'), norm=plt.Normalize(vmin=0, vmax=len(bodyparts) - 1))\n",
        "    sm._A = []\n",
        "    cbar = plt.colorbar(sm, ax=plt.gca(), ticks=range(len(bodyparts)))\n",
        "    cbar.set_ticklabels(bodyparts)\n",
        "\n",
        "    # Extract file name from file path\n",
        "    file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    # Specify the output folder for saving the plot\n",
        "    output_path = os.path.join(output_folder, file_name + '_xyplot.svg')\n",
        "\n",
        "    # Save the plot as an SVG file with the appropriate file name in the specified output folder\n",
        "    plt.savefig(output_path, format='svg')\n",
        "\n",
        "    # Show the plot\n",
        "    plt.close()  # Close the figure to prevent overlapping plots"
      ],
      "metadata": {
        "id": "WFLfZC0JmLyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over each H5 file in the folder\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.h5'):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        plot_xy_from_h5(file_path, output_folder)"
      ],
      "metadata": {
        "id": "-aEz7O4HmNxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calculating total distances https://github.com/farhanaugustine/DeepLabCut-Analysis-Jupyter-Scripts/blob/main/DLC_ROI_Distance_Velocity_Entries.ipynb"
      ],
      "metadata": {
        "id": "i1o5Qv4rXrKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#All imports\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "oXerYMByXuGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###ATTENTION: Variables to set###\n",
        "input_directory = '/content/drive/MyDrive/BEHAVIOMICS/SpecificAim2/20240502_two_cichlid_analyses/formatted'\n",
        "output_directory = '/content/drive/MyDrive/BEHAVIOMICS/SpecificAim2/20240502_two_cichlid_analyses/distance'\n",
        "confidence = 0.4 #confidence threshold of how confident you want the bodyparts to be to be considered for calculation\n",
        "max_distance_per_frame = 100  #Set distance in pixels that the bodypart can jump before being considered as erroneous\n",
        "frame_rate = 30  #Frame rate of your videos"
      ],
      "metadata": {
        "id": "I3Bkd-lB72UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to detect jumps\n",
        "def detect_jumps(x_coords, y_coords, max_distance):\n",
        "    jumps = []\n",
        "    for i in range(1, len(x_coords)):\n",
        "        distance = np.sqrt((x_coords[i] - x_coords[i-1])**2 + (y_coords[i] - y_coords[i-1])**2)\n",
        "        if distance > max_distance:\n",
        "            jumps.append(i)\n",
        "    return jumps\n",
        "\n",
        "# Define the function to calculate filtered distance\n",
        "def calculate_filtered_distance(body_part_data, part_name, max_distance_per_frame):\n",
        "    total_distance = 0\n",
        "    x_coords = body_part_data[part_name]['x']\n",
        "    y_coords = body_part_data[part_name]['y']\n",
        "    jumps = detect_jumps(x_coords, y_coords, max_distance_per_frame)\n",
        "\n",
        "    # Iterate over consecutive indices, excluding the ones identified as jumps\n",
        "    i = 0\n",
        "    while i < len(x_coords) - 1:\n",
        "        if i not in jumps and i + 1 not in jumps:\n",
        "            x_diff = x_coords[i + 1] - x_coords[i]\n",
        "            y_diff = y_coords[i + 1] - y_coords[i]\n",
        "            if not (np.isnan(x_diff) or np.isnan(y_diff)):\n",
        "                distance = np.sqrt(x_diff**2 + y_diff**2)\n",
        "                total_distance += distance\n",
        "            i += 1  # Move to the next valid index\n",
        "        else:\n",
        "            # Skip the consecutive indices identified as jumps\n",
        "            i += 2\n",
        "    return total_distance, jumps\n",
        "\n",
        "# Define the list of body parts to analyze\n",
        "body_parts = [\n",
        "    'eyes_left',\n",
        "    'eyes_right',\n",
        "    'mouthend_left',\n",
        "    'mouthend_right',\n",
        "    'caudalfin_top_base',\n",
        "    'caudalfin_bottom_base',\n",
        "    'caudalfin_top_end',\n",
        "    'caudalfin_bottom_end',\n",
        "    'dorsal_opercular',\n",
        "    'dorsal_opercular_mid',\n",
        "    'dorsal_mid',\n",
        "    'dorsal_midbase',\n",
        "    'ventral_opercular',\n",
        "    'ventral_opercular_mid',\n",
        "    'ventral_mid',\n",
        "    'ventral_midbase',\n",
        "    'pectoralfin_base_left',\n",
        "    'pectoralfin_base_right',\n",
        "]"
      ],
      "metadata": {
        "id": "XEHqx3by8KRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over each CSV file in the input directory\n",
        "for filename in os.listdir(input_directory):\n",
        "    if filename.endswith(\".csv\"):\n",
        "        unpickle = os.path.join(input_directory, filename)\n",
        "\n",
        "        # Read the CSV file\n",
        "        df = pd.read_csv(unpickle, index_col=0, header=list(range(3)))\n",
        "\n",
        "        # Process the data\n",
        "        for col_name in df.columns:\n",
        "            if \"likelihood\" in col_name[2]:\n",
        "                x_col = (col_name[0], col_name[1], 'x')\n",
        "                y_col = (col_name[0], col_name[1], 'y')\n",
        "                df.loc[df[col_name] < confidence, [x_col, y_col]] = float('nan')\n",
        "\n",
        "        body_part_data = {}\n",
        "        unique_body_parts = df.columns.get_level_values(1).unique().tolist()\n",
        "        scorer = df.columns.get_level_values(0)[0]\n",
        "\n",
        "        for part in unique_body_parts:\n",
        "            body_part_data[part] = {\n",
        "                \"x\": df.loc[:, (scorer, part, 'x')].to_numpy(),\n",
        "                \"y\": df.loc[:, (scorer, part, 'y')].to_numpy(),\n",
        "                \"likelihood\": df.loc[:, (scorer, part, 'likelihood')].to_numpy()\n",
        "            }\n",
        "\n",
        "        distances = {}\n",
        "        jumps_detected = {}\n",
        "\n",
        "        for part in body_parts:\n",
        "            distances[part], jumps_detected[part] = calculate_filtered_distance(body_part_data, part, max_distance_per_frame)\n",
        "\n",
        "        total_filtered_distance_moved_in_units = sum(distances.values())\n",
        "\n",
        "        # Convert the dictionary to a pandas DataFrame\n",
        "        df_output = pd.DataFrame(distances.items(), columns=['Body Part', 'Distance'])\n",
        "\n",
        "        # Save the DataFrame to a CSV file with the original filename appended with \"distance\"\n",
        "        output_filename = os.path.join(output_directory, filename.split('.')[0] + \"_distance.csv\")\n",
        "        df_output.to_csv(output_filename, index=False)"
      ],
      "metadata": {
        "id": "7J0AJCJ_8NYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calculating the number of butts\n",
        "\n",
        "Factors included:\n",
        "*   Confidence of the body part\n",
        "*   In region of interest defined\n",
        "*   Whether the cichlid is moving towards the divider\n",
        "*   Whether the velocity is above a defined level\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ux9jXWvbxz9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###ATTENTION: run on laptop command prompt/terminal to find region of interest(ROI) for your video\n",
        "#colab doesn't support GUI, to define ROI in a randomly extracted frame of your video (should match the pickle file you set)\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Polygon\n",
        "\n",
        "class ROIPolygon:\n",
        "    def __init__(self, image):\n",
        "        self.image = image\n",
        "        self.fig, self.ax = plt.subplots()\n",
        "        self.ax.set_title('Select ROI')\n",
        "        self.ax.imshow(self.image, cmap='gray')\n",
        "        self.ROI_coords = []\n",
        "\n",
        "    def select_roi(self):\n",
        "        print(\"Select points to define the ROI polygon. Press 'Enter' to finish.\")\n",
        "        self.ROI_coords = plt.ginput(n=-1, timeout=-1)\n",
        "        polygon = Polygon(self.ROI_coords, closed=True, fill=None, edgecolor='r')\n",
        "        self.ax.add_patch(polygon)\n",
        "        plt.show()\n",
        "\n",
        "video_path = r\"C:\\Users\\2m_separated5.MOV\" #Location of your video\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "random_frame_index = np.random.randint(0, total_frames)\n",
        "cap.set(cv2.CAP_PROP_POS_FRAMES, random_frame_index)\n",
        "ret, frame = cap.read()\n",
        "frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "cap.release()\n",
        "ROI = ROIPolygon(frame_rgb)\n",
        "ROI.select_roi()"
      ],
      "metadata": {
        "id": "RrOKbYuXx4lQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###ATTENTION: Variables to set###\n",
        "velocity = '/content/drive/MyDrive/2m_separated4_left_formatted_vel.csv' #csv file of the velocity file\n",
        "unpickle = '/content/drive/MyDrive/2m_separated4_left_formatted.csv' #the one with the correct header, csv file\n",
        "confidence = 0.4 #number between 0 - 1\n",
        "directory_path = '/content/drive/MyDrive/butts'#where to save the files\n",
        "\n",
        "# Define the bounding box coordinates ROI (x,y)\n",
        "coord1 = (809, 375)  # Top-left corner\n",
        "coord2 = (1103, 387)  # Top-right corner\n",
        "coord3 = (1114, 1030)  # Bottom-right corner\n",
        "coord4 = (793, 1022)  # Bottom-left corner\n",
        "#Record the coordinates you used into a sheet with the relevant file names, so you can refer back to it later & used to verify\n",
        "\n",
        "vel_thresh = 1 #above this velocity is considered as butt (usually 0.0 - 30.0 pixels per second)\n",
        "vel_wndw = 40 #how many frames velocity_rolling_window 0-50 (based on 30frames per second)\n",
        "side = 'left' #which side of the tank is cichlid on, either 'left' or 'right'"
      ],
      "metadata": {
        "id": "tLhNMYUdx6ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#All imports\n",
        "import pandas as pd\n",
        "from shapely.geometry import Polygon\n",
        "from shapely.geometry import Point\n",
        "import os"
      ],
      "metadata": {
        "id": "8TvI3RX1x2I7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cleans up data input by confidence level set for bodyparts\n",
        "bounding_box_polygon = Polygon([coord1, coord2, coord3, coord4]) #Polygon defined\n",
        "df_vel = pd.read_csv(velocity, index_col = 0, header=list(range(3)))\n",
        "for col_name in df_vel.columns:\n",
        "    if \"likelihood\" in col_name[2]:\n",
        "      print(col_name[1])\n",
        "      x_col = (col_name[0], col_name[1], 'x')\n",
        "      y_col = (col_name[0], col_name[1], 'y')\n",
        "      df_vel.loc[df_vel[col_name] < confidence, [x_col, y_col]] = float('nan')\n",
        "\n",
        "print(df_vel)\n",
        "\n",
        "unpickle_df = pd.read_csv(unpickle, index_col = 0, header=list(range(3)))\n",
        "for col_name in unpickle_df.columns:\n",
        "    if \"likelihood\" in col_name[2]:\n",
        "      print(col_name[1])\n",
        "      x_col = (col_name[0], col_name[1], 'x')\n",
        "      y_col = (col_name[0], col_name[1], 'y')\n",
        "      unpickle_df.loc[unpickle_df[col_name] < confidence, [x_col, y_col]] = float('nan')\n",
        "\n",
        "print(unpickle_df)"
      ],
      "metadata": {
        "id": "hdsSd5hpx_e0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Runs through how butts are determined: direction, velocity, location\n",
        "#Saves csv of butts for each bodypart across frames\n",
        "butts_per_bodypart_df = pd.DataFrame(index=unpickle_df.index, columns=unpickle_df.columns.levels[1])\n",
        "prev_x_list = []\n",
        "for column in unpickle_df.columns:\n",
        "    if 'x' in column:\n",
        "        x_column = column\n",
        "        y_column = (column[0], column[1], 'y')\n",
        "        for index, row in unpickle_df.iterrows():\n",
        "            if bounding_box_polygon.contains(Point(row[x_column], row[y_column])):\n",
        "                if prev_x_list:\n",
        "                    if side == \"left\":\n",
        "                        if any(prev_x < row[x_column] for prev_x in prev_x_list):\n",
        "                            x_vel_column = ('vscc', column[1], 'x')\n",
        "                            y_vel_column = ('vscc', column[1], 'y')\n",
        "                            x_vel = df_vel.at[index, x_vel_column]\n",
        "                            y_vel = df_vel.at[index, y_vel_column]\n",
        "                            if x_vel > vel_thresh and y_vel > vel_thresh:\n",
        "                                butts_per_bodypart_df[column[1]][index] = 1\n",
        "                                print(column[1], index, 'butts')\n",
        "                    elif side == \"right\":\n",
        "                        if any(prev_x > row[x_column] for prev_x in prev_x_list):\n",
        "                            x_vel_column = ('vscc', column[1], 'x')\n",
        "                            y_vel_column = ('vscc', column[1], 'y')\n",
        "                            x_vel = df_vel.at[index, x_vel_column]\n",
        "                            y_vel = df_vel.at[index, y_vel_column]\n",
        "                            if x_vel > vel_thresh and y_vel > vel_thresh:\n",
        "                                butts_per_bodypart_df[column[1]][index] = 1\n",
        "                                print(column[1], index, 'butts')\n",
        "                prev_x_list.append(row[x_column])\n",
        "                prev_x_list = prev_x_list[-vel_wndw:]\n",
        "butts_per_bodypart_df.to_csv(os.path.join(directory_path, f'{os.path.splitext(os.path.basename(unpickle))[0]}_butts_bp.csv'))"
      ],
      "metadata": {
        "id": "7vIFtQkayCi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert bodyparts file to one column detecting butts based on if any bodyparts detects butts\n",
        "#Saves csv of combined butts if any body part detects it across frames\n",
        "#Saves csv of frames converted to seconds (30fps) if any block of 30 frames detects a butts, then the second is assigned as butts\n",
        "butts_df = pd.DataFrame(index=butts_per_bodypart_df.index, columns=['butts'])\n",
        "for index, row in butts_per_bodypart_df.iterrows():\n",
        "    if 1 in row.values:\n",
        "        butts_df.at[index, 'butts'] = 1\n",
        "    else:\n",
        "        butts_df.at[index, 'butts'] = 0\n",
        "butts_30fps = butts_df.groupby(butts_df.index // 30).any().astype(int)\n",
        "total_butts = butts_30fps['butts'].sum()\n",
        "print(butts_30fps)\n",
        "print(\"total number of butts:\", (total_butts))\n",
        "butts_df.to_csv(os.path.join(directory_path, f'{os.path.splitext(os.path.basename(unpickle))[0]}_butts.csv'))\n",
        "butts_30fps.to_csv(os.path.join(directory_path, f'{os.path.splitext(os.path.basename(unpickle))[0]}_butts_30fps.csv'))"
      ],
      "metadata": {
        "id": "cWRfTF_EySOC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
